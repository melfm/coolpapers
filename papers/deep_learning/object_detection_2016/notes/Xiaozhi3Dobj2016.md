# 3D Object Proposals using Stereo Imagery for Accurate Object Class Detection
- 3D object proposal by exploiting stereo imagery
- Minimize energy function that :
    - encodes object size priors
    - placement of objects on the ground plane
    - several depth informed features that reason about free space
    - point cloud densities and distance to the ground
- exploit a CNN on top of these proposals to do object detection
- CNN exploits context and depth information to jointly regress to 3D bounding box coord and object pose
- experiment with LIDAR information, combining LIDAR and stereo
- 3D info estimated from a stereo camera pair by placing 3D candidate boxes on the ground plane and scoring
them via 3D point cloud features.
- The scoring function encodes several depth informed features such as point densities inside a candidate box, heights etc
- Learning done using structured SVM to obstain class-specific weights for features
- Neural net takes 3D object proposals and predicts 3D bounding boxes, using contextual info and a
multi-task loss to jointly regress to bounding box coordinates and object orientation (multi-task loss?)
- 3D object proposal model is extended with a class-independent variant
- Detection neural net extened to a two-stream network to leverage both appearance and depth features.
- R-CNN is applied on pedestrian detection with proposals generated by SquaresChnFtrs detector
- Faster R-CNN integrates proposal generation and R-CNN into an end-to-tend trainable net
- All these methods work on 2D but we want 3D to infer both depth and object pose

## Proposed approach
- Input is stereo image pair, depth is computed using method x which gives a point cloud
- Place object proposals in 3D space in the form of 3D bounding boxes
- Only depth info is used and no appearance (image?)

### Proposal generation as energy minimization
- 3D bounding box to represent each object proposal $y$, parametrized by a tuple $(x,y,z,\theta,c,t)$
$(x,y,z)$ is the 3D box center and $\theta$ denotes the azimuth angle. $c \in C$ is object class and
$t \in {1,...,T_c}$ indexes a set of 3D box templates which are learnt
- Discretize the 3D space into voxels for candidate box sampling
- Generate proposals by minimizing an energy function which encodes several depth-informed potentials
- Encode the fact that the object should live in a space occupied with high density by the point cloud.
- The weights of the energy terms are learnt via structured SVM
- Section 3.1 goes through encoding procedure for point cloud density, free space, heigh etc
    - Free Space : space th at lies on the rays between the point cloud and camera

### Inference
- Minimize the energy $y^* = argmin_y E(x,y)$.
- Point cloud is computed from a stereo image pair
- 3D space is discretized and road plane for 3D candidate box sampling is estimated
- Finally perform exhaustive scoring and non-maximal suppression (NMS) to obstain 3D proposals.

- Discretization and Accumulators: Point cloud defined in a left-handed coordinate system, y-axis goes in the direction of
gravity and the positive z-axis is along the cameras viewing direction.
- Ground plane estimation: estimate ground plane by classifying superpixels using a small neural net and fitting a plane
to the estimated ground pixels using RANSAC. Using features: mean RGB values, average 2D and 3D position, pitch and roll angles relative
to the camera of the plane fit to the superpixel, flag to whether the average 2D position was above the horizon line and s.d. of both
the color values and 3D position -> 22-dimensional feature vector. The net has 22 hidden units and $tanh$ activation and cross-entropy as the loss function.
- Bounding boxes sampling and scoring: For 3D candidate box sampling, use three size templates per class and two orientations $\theta \in{0,90}$.
As all the features can be efficiently computed via integral accumulators, it takes constant time to evaluate each configuration $y$.
Still evaluating exhaustively in the entire search space would be slow, reduce the search space by skipping empty boxes. With ground
plane estimation, futher reduce the search space along the vertical dimension by only placing cadidate boxes on the ground plane.

### 3D object detection network
- Design a net for two tasks: joint 2D object detection and orientation estimation and 3D object detection.
- for 2D object detection and orientation, the net is built upon Fast R-CNN which share the convolutional features across all proposals
and use a ROI pooling layer to compute proposal-specific features.
- Extend this basic net by adding a context branch after the last convolutional layer (conv5) and an orientation regression loss to jointly
learn object location and orientation.
- Same network shown is used except that 2D bounding box regressors are replaced by 3D bounding box regressors
- Parametrize the centers of 3D boxes with size normalization for scale-invariant translation and 3D box sizes with log-space shift.
- Given the 3D box coordinates and the estimated orientation, we compute the azimuth angle of the box.
- CNN scoring with depth features: Despite only using RGB image with basic network it performs well in practice.
To take advantage of depth info in CNN scoring process, further compute a depth image encoded with HHA features.
- HHA has three channels with represent the disparity map, height above the ground, the angle of the normal at each pixel w.r.t gravity.
- Two approaches to learn feature representation with both RGB and depth images as input.
    - First approach is a single-stream network which directly combines RGB channels and HHA channels to form a 6-channel image fed to net.
    - Second approach is a two-stream network that learns features from RGB and HHA images respectively.

### Implementation
- Class specific weights used for proposal generation
- For network training, choose training samples based on the IoU overlap threshold of the 2D bounding box proposals and ground truth boxes.
- By default using VGG-16 network trained on ImageNet to initilize the network.
- initialize the context branch by copying the weights of the fully-connected layers from the pre-trained model.
- Upscale the input image by a factor of 3.5 which is important to achieve very good performance
- Run SGD and set the initial learning rate to 0.001.
- After 30K iterations reduce it to 0.0001 and run another 10K.
- Training proposals are sampled in a image-centric manner with batch size of 1 for images and 128 for proposals.
- Note the net takes around 2s to evaluate one image with 2K proposal on GPU!





