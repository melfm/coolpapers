# Distribution Similarity

Many problems in testing and learning require evaluating distribution similarity in high dimensions.
The analysis of distributions is fundamental. Many algorithms rely on information theoretic approaches such as entropy, mutual information, or Kullbackâ€“Leibler divergence. However, to estimate these quantities,one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data.


## [Minimax Estimation of Maximum Mean Discrepancywith Radial Kernels](https://papers.nips.cc/paper/6483-minimax-estimation-of-maximum-mean-discrepancy-with-radial-kernels.pdf)

### Notes
- A family of divergences are the integral probability metrics, they find a witness function to distinguish samples from `P` and `Q`.
- The distance MMD measures is based on the notion of embedding probabilities in a reproducing kernel Hilbert space.
- Instead of density estimators, use means directly
- 


## [Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy](https://arxiv.org/abs/1611.04488)

### Notes


### Umbrella Techniques
- Jensen-Shannon divergence measure 
- Kernel embedding of distributions
- 


